> # Sequence MODELS:

1. `RNN & LSTM`

    a. Backpropagation Through Time

    b. Vanishing Gradients with RNNs

    c. LSTM

2. `Natural Language Processing & Word Embeddings`

    a. Word Embeddings:

        i. Word2Vec
        ii. Skip grams
        iii. Negative Sampling
        iv. GloVe

3. `Sequence Models & Attention Mechanism`

    a. Beam Search

    b. Attention Model

4. `Transformer Network`

    a. Self-Attention

    b. Multi-Head Attention

    c. Transformer Network

> # ATTENTION MODEL:

1. `Neural Machine Translation`

    a. Queries, Keys, Values, and Attention

    b. Teacher Forcing

    c. Beam Search

2. `Text Summarization`

    a. Transformer Encoder:

        i. Maksked self-attention
        ii. Multi-head attention

    b. Transformer Decoder

3. `Question Answering`

    a. BERT

    b. T5

    c. GLUE benchmark

    d. Hugging face

4. `Chatbot`

    a. LSH(Locality sensitive hashing) Attention - resolve expensive computation of attention dot products

    b. Reversible Layer - resolve LLM backpropagation memory bottle-neck

    c. Reformer: LSH + Reversible Layer